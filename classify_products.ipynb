{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bac923f",
   "metadata": {},
   "source": [
    "# Products classification through AI\n",
    "In this notebook, I get the products information contained in the csv file and the intention is to classify each one as Hazmat (Hazardous Material) or not. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9eef94",
   "metadata": {},
   "source": [
    "## Classify products based on the data obtained (title and attributes from ML API)\n",
    "Given that I am using Groq/Gemini for free tier, I'll classify the products in batches of 50 products per LLM call. The amount of products in the same batch must be optimized for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f05e9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from enum import Enum\n",
    "\n",
    "# Important definitions\n",
    "\n",
    "class Confidence(Enum):\n",
    "    LOW = \"low\"\n",
    "    MEDIUM = \"medium\"\n",
    "    HIGH = \"high\"\n",
    "\n",
    "class HazmatClassification(BaseModel):\n",
    "    product_id: str = Field(..., description=\"The unique identifier of the product.\")\n",
    "    is_hazmat: bool = Field(..., description=\"Indicates whether the product is classified as a Hazmat.\")\n",
    "    reason: str = Field(None, description=\"The reason for the classification, if the product is a Hazmat.\")\n",
    "    confidence: Confidence = Field(None, description=\"The confidence level of the classification, if the product is a Hazmat.\")\n",
    "\n",
    "DATASET = 'dataset_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6260c831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hazmat definition from validated file\n",
    "with open(\"data/hazmat-definition.md\", \"r\", encoding='utf8') as f:\n",
    "    hazmat_def = f.read()\n",
    "\n",
    "# Get products information from csv file\n",
    "import pandas as pd\n",
    "products_df = pd.read_csv(f\"data/{DATASET}/{DATASET}.csv\") \n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# TEMPORARY: Get products without classification (Until 2219, the batch size was 30 and gemini-2.5-flash was used)\n",
    "products_df = products_df.iloc[6020:] # Now it is 100 products per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a1ab0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "hazmat_classifier_system_msg = f\"\"\"\n",
    "You are a domain-expert Hazmat classifier. Your task is to analyze the products below and determine, for each, if it is Hazmat or not, based on the definition provided between <hazmat_definition> tags.\n",
    "\n",
    "You must base your analysis on the following JSON schema, which describes the required analysis for each product in the fields:\n",
    "<json_schema>{HazmatClassification.model_json_schema()}</json_schema>\n",
    "\n",
    "Before answering, you must output your detailed reasoning process.\n",
    "\n",
    "Hazmat definition: <hazmat_definition>{hazmat_def}</hazmat_definition>\n",
    "\n",
    "Guidelines:\n",
    "- Always refer to the Hazmat definition to address the classification. Do not suppose anything. If not certain of the classification, output as hazmat with lower confidence.\n",
    "- Only output a product as non-hazmat if you are absolutely certain that it is not a Hazmat according to the definition provided.\n",
    "\"\"\"\n",
    "\n",
    "hazmat_json_extractor_system_msg = f\"\"\"\n",
    "You are a domain-expert Hazmat classifier. Based on the analysis below, extract and output the final answer as a jsonl structure, located between <jsonl> tags, with each line following this schema (one line per product): <json_schema>{HazmatClassification.model_json_schema()}</json_schema>.\n",
    "\n",
    "Guidelines:\n",
    "- For the tag <jsonl>: The final answer must be a valid jsonl structure, with each line following the schema provided.\n",
    "- If not certain of the classification, output as hazmat with lower confidence.\n",
    "- Only output a product as non-hazmat if you are absolutely certain that it is not a Hazmat according to the definition provided.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed158056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from defs_and_tools import call_llm, extract_from_tag\n",
    "import requests\n",
    "from docling.document_converter import DocumentConverter\n",
    "from html_to_markdown import convert_to_markdown\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# json_extractor_models = [\"groq/llama-3.3-70b-versatile\",\n",
    "#                         \"groq/llama3-70b-8192\",\n",
    "#                         \"gemini/gemini-2.0-flash\"]\n",
    "# json_extractor_model = \"gemini/gemini-2.0-flash\" # Did not create the tags correctly for output parsing\n",
    "json_extractor_model = \"gemini/gemini-2.5-flash\"\n",
    "hazmat_classifier_model = \"gemini/gemini-2.5-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f2e9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 1 jsonl content extracted!\n",
      "Batch 1 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 2 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 2 jsonl content extracted!\n",
      "Batch 2 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 3 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 3 jsonl content extracted!\n",
      "Batch 3 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 4 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 4 jsonl content extracted!\n",
      "Batch 4 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 5 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 5 jsonl content extracted!\n",
      "Batch 5 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 6 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 6 jsonl content extracted!\n",
      "Batch 6 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 7 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 7 jsonl content extracted!\n",
      "Batch 7 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 8 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 8 jsonl content extracted!\n",
      "Batch 8 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 9 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 9 jsonl content extracted!\n",
      "Batch 9 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 10 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 10 jsonl content extracted!\n",
      "Batch 10 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 11 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 11 jsonl content extracted!\n",
      "Batch 11 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 12 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 12 jsonl content extracted!\n",
      "Batch 12 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 13 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 13 jsonl content extracted!\n",
      "Batch 13 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 14 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 14 jsonl content extracted!\n",
      "Batch 14 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 15 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 15 jsonl content extracted!\n",
      "Batch 15 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 16 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 16 jsonl content extracted!\n",
      "Batch 16 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 17 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 17 jsonl content extracted!\n",
      "Batch 17 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 18 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 18 jsonl content extracted!\n",
      "Batch 18 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 19 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 19 jsonl content extracted!\n",
      "Batch 19 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 20 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 20 jsonl content extracted!\n",
      "Batch 20 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 21 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 21 jsonl content extracted!\n",
      "Batch 21 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 22 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 22 jsonl content extracted!\n",
      "Batch 22 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 23 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 23 jsonl content extracted!\n",
      "Batch 23 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 24 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 24 jsonl content extracted!\n",
      "Batch 24 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 25 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 25 jsonl content extracted!\n",
      "Batch 25 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 26 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 26 jsonl content extracted!\n",
      "Batch 26 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 27 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 27 jsonl content extracted!\n",
      "Batch 27 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 28 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 28 jsonl content extracted!\n",
      "Batch 28 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 29 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 29 jsonl content extracted!\n",
      "Batch 29 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 30 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 30 jsonl content extracted!\n",
      "Batch 30 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 31 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 31 jsonl content extracted!\n",
      "Batch 31 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 32 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 32 jsonl content extracted!\n",
      "Batch 32 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 33 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 33 jsonl content extracted!\n",
      "Batch 33 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 34 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 34 jsonl content extracted!\n",
      "Batch 34 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 35 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 35 jsonl content extracted!\n",
      "Batch 35 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 36 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 36 jsonl content extracted!\n",
      "Batch 36 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 37 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 37 jsonl content extracted!\n",
      "Batch 37 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 38 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 38 jsonl content extracted!\n",
      "Batch 38 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 39 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 39 jsonl content extracted!\n",
      "Batch 39 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 40 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 40 jsonl content extracted!\n",
      "Batch 40 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 41 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 41 jsonl content extracted!\n",
      "Batch 41 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 42 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 42 jsonl content extracted!\n",
      "Batch 42 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 43 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 43 jsonl content extracted!\n",
      "Batch 43 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 44 with 100 products...\n",
      "Raw response received, formatting to JSONL...\n",
      "Batch 44 jsonl content extracted!\n",
      "Batch 44 processed and saved to data/dataset_1/dataset_1_classified_products.jsonl and data/dataset_1/dataset_1_raw_log.txt!\n",
      "----------------------------------------\n",
      "Processing batch 45 with 100 products...\n"
     ]
    }
   ],
   "source": [
    "def classify_products(products_df, batch_size=30, output_jsonl=\"classified_products.jsonl\", log_file=\"log_file.txt\"):\n",
    "    \"\"\"Classify products in batches and save results.\"\"\"\n",
    "\n",
    "    for i in range(0, len(products_df), batch_size):\n",
    "        batch = products_df.iloc[i:i + batch_size]\n",
    "        batch_list = batch.to_dict(orient=\"records\")\n",
    "        \n",
    "        print(f\"Processing batch {i//batch_size + 1} with {len(batch_list)} products...\")\n",
    "        raw_response = call_llm(\n",
    "            system=hazmat_classifier_system_msg,\n",
    "            prompt=f\"Products to classify:\\n{batch_list}\",\n",
    "            model=hazmat_classifier_model,\n",
    "        )\n",
    "        \n",
    "        print(\"Raw response received, formatting to JSONL...\")\n",
    "        formatted_response = call_llm(\n",
    "            system=hazmat_json_extractor_system_msg,\n",
    "            prompt=raw_response,\n",
    "            model=json_extractor_model,\n",
    "        )\n",
    "        \n",
    "        # Save JSONL output\n",
    "        jsonl_content = extract_from_tag(formatted_response, \"jsonl\")\n",
    "        if jsonl_content:\n",
    "            print(f\"Batch {i//batch_size + 1} jsonl content extracted!\")\n",
    "            with open(output_jsonl, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(jsonl_content + \"\\n\")\n",
    "        \n",
    "        # Save raw log\n",
    "        with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"Batch {i//batch_size + 1}:\\n{raw_response}\\n\\n\")\n",
    "        \n",
    "        print(f\"Batch {i//batch_size + 1} processed and saved to {output_jsonl} and {log_file}!\")\n",
    "        print(40*\"-\")\n",
    "\n",
    "classify_products(products_df, \n",
    "                  output_jsonl=f\"data/{DATASET}/{DATASET}_classified_products.jsonl\",\n",
    "                  log_file=f\"data/{DATASET}/{DATASET}_raw_log.txt\",\n",
    "                  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf46b7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Prompt: Open jsonl file and insert result into dataframe products_df\n",
    "\n",
    "# Read the classified products JSONL file and insert results into products_df\n",
    "jsonl_path = f\"data/{DATASET}/{DATASET}_classified_products.jsonl\"\n",
    "classified_rows = []\n",
    "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            classified_rows.append(json.loads(line))\n",
    "\n",
    "classified_df = pd.DataFrame(classified_rows)\n",
    "\n",
    "# Merge classified_df into products_df on 'product_id'\n",
    "products_df = products_df.merge(classified_df, on=\"product_id\", how=\"left\", suffixes=(\"\", \"_classified\"))\n",
    "\n",
    "products_df.head()\n",
    "# Save the updated products_df with classifications\n",
    "products_df.to_csv(f\"data/{DATASET}/{DATASET}_classified_products.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
