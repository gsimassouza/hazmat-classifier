{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bac923f",
   "metadata": {},
   "source": [
    "# Products classification through AI\n",
    "In this notebook, I get the products information contained in the csv file and the intention is to classify each one as Hazmat (Hazardous Material) or not. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9eef94",
   "metadata": {},
   "source": [
    "## Classify products based on the data obtained (title and attributes from ML API)\n",
    "Given that I am using Groq/Gemini for free tier, I'll classify the products in batches of 50 products per LLM call. The amount of products in the same batch must be optimized for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f05e9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from enum import Enum\n",
    "\n",
    "# Important definitions\n",
    "\n",
    "class Confidence(Enum):\n",
    "    LOW = \"low\"\n",
    "    MEDIUM = \"medium\"\n",
    "    HIGH = \"high\"\n",
    "\n",
    "class HazmatClassification(BaseModel):\n",
    "    product_id: str = Field(..., description=\"The unique identifier of the product.\")\n",
    "    is_hazmat: bool = Field(..., description=\"Indicates whether the product is classified as a Hazmat.\")\n",
    "    reason: str = Field(None, description=\"The reason for the classification, if the product is a Hazmat.\")\n",
    "    confidence: Confidence = Field(None, description=\"The confidence level of the classification, if the product is a Hazmat.\")\n",
    "\n",
    "DATASET = 'dataset_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6260c831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hazmat definition from validated file\n",
    "with open(\"data/hazmat-definition.md\", \"r\", encoding='utf8') as f:\n",
    "    hazmat_def = f.read()\n",
    "\n",
    "# Get products information from csv file\n",
    "import pandas as pd\n",
    "products_df = pd.read_csv(f\"data/{DATASET}/{DATASET}.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a1ab0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "hazmat_classifier_system_msg = f\"\"\"\n",
    "You are a domain-expert Hazmat classifier. Your task is to analyze the products below and determine, for each, if it is Hazmat or not, based on the definition provided between <hazmat_definition> tags.\n",
    "\n",
    "You must base your analysis on the following JSON schema, which describes the required analysis for each product in the fields:\n",
    "<json_schema>{HazmatClassification.model_json_schema()}</json_schema>\n",
    "\n",
    "Before answering, you must output your detailed reasoning process.\n",
    "\n",
    "Hazmat definition: <hazmat_definition>{hazmat_def}</hazmat_definition>\n",
    "\n",
    "Guidelines:\n",
    "- Always refer to the Hazmat definition to address the classification. Do not suppose anything. If not certain of the classification, output as hazmat with lower confidence.\n",
    "- Only output a product as non-hazmat if you are absolutely certain that it is not a Hazmat according to the definition provided.\n",
    "\"\"\"\n",
    "\n",
    "hazmat_json_extractor_system_msg = f\"\"\"\n",
    "You are a domain-expert Hazmat classifier. Based on the analysis below, extract and output the final answer as a jsonl structure, located between <jsonl> tags, with each line following this schema (one line per product): <json_schema>{HazmatClassification.model_json_schema()}</json_schema>.\n",
    "\n",
    "Guidelines:\n",
    "- For the tag <jsonl>: The final answer must be a valid jsonl structure, with each line following the schema provided.\n",
    "- If not certain of the classification, output as hazmat with lower confidence.\n",
    "- Only output a product as non-hazmat if you are absolutely certain that it is not a Hazmat according to the definition provided.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed158056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from defs_and_tools import call_llm, extract_from_tag\n",
    "import requests\n",
    "from docling.document_converter import DocumentConverter\n",
    "from html_to_markdown import convert_to_markdown\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# json_extractor_models = [\"groq/llama-3.3-70b-versatile\",\n",
    "#                         \"groq/llama3-70b-8192\",\n",
    "#                         \"gemini/gemini-2.0-flash\"]\n",
    "json_extractor_model = \"gemini/gemini-2.0-flash\"\n",
    "hazmat_classifier_model = \"gemini/gemini-2.5-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f2e9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 with 30 products...\n"
     ]
    }
   ],
   "source": [
    "def classify_products(products_df, batch_size=30, output_jsonl=\"classified_products.jsonl\", log_file=\"log_file.txt\"):\n",
    "    \"\"\"Classify products in batches and save results.\"\"\"\n",
    "\n",
    "    for i in range(0, len(products_df), batch_size):\n",
    "        batch = products_df.iloc[i:i + batch_size]\n",
    "        batch_list = batch.to_dict(orient=\"records\")\n",
    "        \n",
    "        print(f\"Processing batch {i//batch_size + 1} with {len(batch_list)} products...\")\n",
    "        raw_response = call_llm(\n",
    "            system=hazmat_classifier_system_msg,\n",
    "            prompt=f\"Products to classify:\\n{batch_list}\",\n",
    "            model=hazmat_classifier_model,\n",
    "        )\n",
    "        \n",
    "        print(\"Raw response received, formatting to JSONL...\")\n",
    "        formatted_response = call_llm(\n",
    "            system=hazmat_json_extractor_system_msg,\n",
    "            prompt=raw_response,\n",
    "            model=json_extractor_model,\n",
    "        )\n",
    "        \n",
    "        # Save JSONL output\n",
    "        jsonl_content = extract_from_tag(formatted_response, \"jsonl\")\n",
    "        if jsonl_content:\n",
    "            print(f\"Batch {i//batch_size + 1} jsonl content extracted!\")\n",
    "            with open(output_jsonl, \"a\") as f:\n",
    "                f.write(jsonl_content + \"\\n\")\n",
    "        \n",
    "        # Save raw log\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"Batch {i//batch_size + 1}:\\n{raw_response}\\n\\n\")\n",
    "        \n",
    "        print(f\"Batch {i//batch_size + 1} processed and saved to {output_jsonl} and {log_file}!\")\n",
    "        print(40*\"-\")\n",
    "\n",
    "classify_products(products_df, \n",
    "                  output_jsonl=f\"data/{DATASET}/{DATASET}_classified_products.jsonl\",\n",
    "                  log_file=f\"data/{DATASET}/{DATASET}_raw_log.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf46b7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Prompt: Open jsonl file and insert result into dataframe products_df\n",
    "\n",
    "# Read the classified products JSONL file and insert results into products_df\n",
    "jsonl_path = f\"data/{DATASET}/{DATASET}_classified_products.jsonl\"\n",
    "classified_rows = []\n",
    "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            classified_rows.append(json.loads(line))\n",
    "\n",
    "classified_df = pd.DataFrame(classified_rows)\n",
    "\n",
    "# Merge classified_df into products_df on 'product_id'\n",
    "products_df = products_df.merge(classified_df, on=\"product_id\", how=\"left\", suffixes=(\"\", \"_classified\"))\n",
    "\n",
    "products_df.head()\n",
    "# Save the updated products_df with classifications\n",
    "products_df.to_csv(f\"data/{DATASET}/{DATASET}_classified_products.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
